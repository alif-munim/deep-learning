{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c8f1558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0a100c66d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2608e08",
   "metadata": {},
   "source": [
    "### Affine Maps\n",
    "The affine map is a function f(x) = Ax + b, where A is a matrix and x, b are vectors. A and b are learnable parameters. Typically, x is the input and we are applying some matrix transformation to get our desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda2a617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# linear transformation from R5 to R3\n",
    "lin = nn.Linear(5,3)\n",
    "print(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9471660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5404, -2.2102,  2.1130, -0.0040,  1.3800],\n",
      "        [-1.3505,  0.3455,  0.5046,  1.8213, -0.1814]])\n"
     ]
    }
   ],
   "source": [
    "# sample data of dimensions 2x5\n",
    "data = torch.randn(2,5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d5b73",
   "metadata": {},
   "source": [
    "In PyTorch, the ith row of the input to A corresponds to the ith row of the mapped output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e333ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4935,  0.5282,  0.1833],\n",
      "        [-0.0047, -0.0912, -0.3908]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# output dimensions of 2x3\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4dbc9b",
   "metadata": {},
   "source": [
    "### Composition\n",
    "Take two affine maps, f(x) = Ax + b and g(x) = Cx + d. <br>\n",
    "We can compose the two affine maps as such: <br>\n",
    "f(x) = A(Cx + d) + b <br>\n",
    "f(x) = ACx + Ad + b <br> \n",
    "f(x) = AC(x) + (Ad + b) <br>\n",
    "<br>\n",
    "Thus, composing affine maps gives you an affine map. Creating long chains of linear affine compositions in a neural network adds no computational power.<br>\n",
    "<br>\n",
    "We introduce non-linearities between the affine layers to build more powerful models. We typically use a few core non-linearities that are easily differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6926bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9515,  0.4057],\n",
      "        [-1.5164,  0.7322]])\n"
     ]
    }
   ],
   "source": [
    "# In PyTorch, most non-linearities are in torch.functional (F)\n",
    "# Non-linearities don't have parameters that are updated during training, as affine maps do\n",
    "data = torch.randn(2,2)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d355359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.4057],\n",
      "        [0.0000, 0.7322]])\n"
     ]
    }
   ],
   "source": [
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b2bca",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Softmax is a special non-linearity that is typically used at the end of a neural network, and returns a probability distribution from a vector of real numbers. <br>\n",
    "<br>\n",
    "If x is a vector of real numbers, then the i'th component of softmax(x) is: <br>\n",
    "exp(x_i) / sigma_j exp(x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7863e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0147, -0.1819,  0.6182,  0.0393,  0.9262])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83380cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.0925, 0.2059, 0.1154, 0.2802])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35931b",
   "metadata": {},
   "source": [
    "There is also log softmax, which computes log probabilities:\n",
    "\n",
    "> The real advantage is in the arithmetic. Log probabilities are not as easy to understand as probabilities (for most people), but every time you multiply together two probabilities (other than 1Ã—1=1), you will end up with a value closer to 0. Dealing with numbers very close to 0 can become unstable with finite precision approximations, so working with logs makes things much more stable and in some cases quicker and easier. Why do you need any more justification than that?\n",
    "\n",
    "https://stats.stackexchange.com/questions/483927/why-are-log-probabilities-useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f3d5eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1840, -2.3806, -1.5804, -2.1594, -1.2724])\n",
      "tensor(-8.5767)\n"
     ]
    }
   ],
   "source": [
    "print(F.log_softmax(data, dim=0))\n",
    "print(F.log_softmax(data, dim=0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f5429",
   "metadata": {},
   "source": [
    "### Objective Functions\n",
    "Objective functions, also known as loss or cost functions, are what neural networks are trained to minimize. To do so, we first pass in an input to the network, and compute the loss of the output. The parameters of the network are updated by taking the derivative of the loss function. <br><br>\n",
    "The motivation behind minimizing the loss function is to help the network generalize better, with low loss values on previously unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38fd64",
   "metadata": {},
   "source": [
    "### Optimization and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ca8da",
   "metadata": {},
   "source": [
    "Tensors know how to compute gradients with respect to the elements used to compute it using the computational graph. Since loss is a tensor, we can compute gradients with respect to the parameters used to compute it. We can then perform gradient updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd592f",
   "metadata": {},
   "source": [
    "### Creating Network Components in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4221f",
   "metadata": {},
   "source": [
    "We can build a network in PyTorch using only affine maps and non-linearities. In the following example, we will also compute a loss function and update parameters through backpropagation. <br><br>\n",
    "\n",
    "The example networm will take in a sparse bag-of-words (BoW) representation and output a probability distribution over the labels \"English\" and \"Spanish\". <br><br>\n",
    "\n",
    "We assign each word in the vocabulary an index. Then, we pass the input through an affine map and perform a log softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275dddd",
   "metadata": {},
   "source": [
    "### Logistic Regression BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc439b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f20790d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "            (\"it is lost on me\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd259a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = {}\n",
    "for sentence, _ in data + test_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_indices:\n",
    "            word_indices[word] = len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a6299b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f7062c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4af32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(sentence, word_indices):\n",
    "    vector = torch.zeros(len(word_indices))\n",
    "    for word in sentence:\n",
    "        vector[word_indices[word]] += 1\n",
    "    return vector.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa13388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_indices):\n",
    "    return torch.LongTensor([label_indices[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9891af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_indices)\n",
    "NUM_LABELS = 2\n",
    "label_indices = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a355489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,  0.0394,  0.0471,\n",
      "         -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,  0.1796, -0.0363,\n",
      "          0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,  0.1344,  0.0406,\n",
      "          0.0631,  0.1465],\n",
      "        [ 0.1860, -0.1301,  0.0245,  0.1464,  0.1421,  0.1218, -0.1419, -0.1412,\n",
      "         -0.1186,  0.0246,  0.1955, -0.1239,  0.1045, -0.1085, -0.1844, -0.0417,\n",
      "          0.1130,  0.1821, -0.1218,  0.0426,  0.1692,  0.1300,  0.1222,  0.1394,\n",
      "          0.1240,  0.0507]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1341, -0.1647], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ec3ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9706, -0.4762]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = data[0]\n",
    "    bow_vector = bow_vectorizer(sample[0], word_indices)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4aa80ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0669, 0.1955], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters())[:, word_indices[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab49cf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 266.54it/s]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    for instance, label in data:\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        bow_vector = bow_vectorizer(instance, word_indices)\n",
    "        target = make_target(label, label_indices)\n",
    "        \n",
    "        log_probs = model(bow_vector)\n",
    "        \n",
    "        loss = loss_fn(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7e0e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1632, -1.8935]])\n",
      "tensor([[-2.6800, -0.0710]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = bow_vectorizer(instance, word_indices)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a88dfd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = F.softmax(log_probs,dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "759558a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0686, 0.9314]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e268e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
