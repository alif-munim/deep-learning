{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021a7d2c",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ee05d",
   "metadata": {},
   "source": [
    "In NLP, your features are words. We encode these words as 'word embeddings,' which are dense vectors of real numbers. Each index in the vector represents a word in your vocabulary. <br><br>\n",
    "\n",
    "An ASCII character representation could be stored, but this does not convey meaning. We can also try a one-hot encoding, but these are typically huge and do not encode for semantic similarity in addition to orthographic similarity. <br><br>\n",
    "\n",
    "Thus, we must generate dense word embeddings. The fundamental assumption underlying these embeddings is the distributional hypothesis of linguistics, stating that words appearing in similar contexts have similar meanings. <br><br>\n",
    "\n",
    "We use a neural network to learn latent semantic attributes, which will form a new dense vector. We use a normalized dot-product to find the cosine-similarity, which gives the angle between the two vectors. These new vectors are called word embeddings, and they efficiently encode semantic information which is not necessarily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a69eed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6a41576d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79577392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_idx = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "# the embedding function takes two args: vocab size, embed dim.\n",
    "embeddings = nn.Embedding(2, 5)\n",
    "\n",
    "# Define index for each word, keys to lookup table\n",
    "lookup_tensor = torch.tensor([word_idx[\"hello\"]], dtype=torch.long)\n",
    "hello_embedding = embeddings(lookup_tensor)\n",
    "print(hello_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00525a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the embedding for hello\n",
    "embeddings(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e409a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1661, -1.5228,  0.3817, -1.0276, -0.5631],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the embedding for world\n",
    "embeddings(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35ffa6",
   "metadata": {},
   "source": [
    "### N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3765c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f39b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c3e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c239130a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['forty', 'When'], 'winters'),\n",
       " (['winters', 'forty'], 'shall'),\n",
       " (['shall', 'winters'], 'besiege'),\n",
       " (['besiege', 'shall'], 'thy'),\n",
       " (['thy', 'besiege'], 'brow,'),\n",
       " (['brow,', 'thy'], 'And'),\n",
       " (['And', 'brow,'], 'dig'),\n",
       " (['dig', 'And'], 'deep'),\n",
       " (['deep', 'dig'], 'trenches'),\n",
       " (['trenches', 'deep'], 'in')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee209b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afbde82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5baf792",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd5fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "638d625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        context_idx = torch.tensor([word_idx[w] for w in context], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_idx)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_idx[target]], dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f44dba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[518.5340082645416, 516.1016693115234, 513.6846833229065, 511.2812280654907, 508.8921465873718, 506.5155596733093, 504.14948534965515, 501.79528069496155, 499.44975066185, 497.1138048171997]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b1d4564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4274,  1.2996, -1.0202, -0.8538, -1.3634, -0.1731,  1.5392, -1.1700,\n",
      "        -1.0086, -1.1237], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.weight[word_idx[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197f6cd",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28886041",
   "metadata": {},
   "source": [
    "Continuous context is provided; a few words before and after the target word (surrounding context). CBOW is non-sequential and not necessarily probabilistic, unliek language modeling. <br><br>\n",
    "\n",
    "Typically used to quickly train word embeddings (pre-training), improving performance. The model tries to minimize the negative log probability of word i given the context C (words before and after word i). This is equivalent to the negative log softmax of the affine map of context word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb22bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb976382",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a80c81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8e720ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45f517da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeb25d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study')]\n"
     ]
    }
   ],
   "source": [
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6adaeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_idx):\n",
    "    idxs = [word_idx[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "793d9531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42, 10, 34, 24])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_context_vector(data[0][0], word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2f7c18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "695550a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7f10ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a46c8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d897c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_idx = make_context_vector(data[0][0], word_idx)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_idx)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_idx[target]], dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bf8499a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230.36214447021484, 225.46933817863464, 223.80172324180603, 222.60078740119934, 221.75739216804504, 221.17100763320923, 220.76076102256775, 220.46780276298523, 220.25217199325562, 220.0878508090973]\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a48324d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4399, -0.5098, -0.6951, -0.6175, -0.6868, -1.4988,  0.6709,  0.7892,\n",
      "         0.0599,  0.8013,  0.4626, -1.2088, -0.1457, -0.0925, -0.6336, -0.4352,\n",
      "        -1.3568,  1.0663, -1.3798, -2.2706], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.weight[word_idx[\"computational\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ee800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
